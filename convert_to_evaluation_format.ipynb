{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/chainer_p36/lib/python3.6/site-packages/chainer/_environment_check.py:91: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "Multiple installations of CuPy package has been detected.\n",
      "You should select only one package from from ['cupy-cuda102', 'cupy-cuda101', 'cupy-cuda100', 'cupy-cuda92', 'cupy-cuda91', 'cupy-cuda90', 'cupy-cuda80', 'cupy'].\n",
      "Follow these steps to resolve this issue:\n",
      "  1. `pip list` to list CuPy packages installed\n",
      "  2. `pip uninstall <package name>` to uninstall all CuPy packages\n",
      "  3. `pip install <package name>` to install the proper one\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  '''.format(name=name, pkgs=pkgs))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xlrd\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(output_dir, log_filename='log'):\n",
    "    best_epoch = None\n",
    "    best_val_accuracy = None\n",
    "    logs = json.load(open(os.path.join(output_dir, log_filename)))\n",
    "    for log in logs:\n",
    "        val_accuracy = log['validation/main/accuracy']\n",
    "        if best_epoch is None or val_accuracy > best_val_accuracy:\n",
    "            best_epoch = log['epoch']\n",
    "            best_val_accuracy = val_accuracy\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUZUKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "model_name = 'mpnn'\n",
    "output_name = model_name + '_output_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import GraphConvPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/3651 [00:00<00:16, 220.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1075/3651 [00:04<00:11, 223.69it/s]RDKit ERROR: [12:22:46] Explicit valence for atom # 11 B, 4, is greater than permitted\n",
      "100%|██████████| 3651/3651 [00:14<00:00, 243.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "      y_pred_[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]]  \\\n",
      "2191                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "1537                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "306                                                 205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "2511                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "2680                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "\n",
      "      t_[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]]  \n",
      "2191                                                 77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "1537                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "306                                                 205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "2511                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "2680                                                205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "label_name = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.], y_pred = [205 205 205 205 205 205 205 205 205 205], t = [205 205 205   0 205 205 205 205 205 205]\n",
      "Evaluating...\n",
      "Evaluation result:  {'main/loss': 0.0638949653402166, 'main/accuracy': 0.9800711219487752}\n"
     ]
    }
   ],
   "source": [
    "# predict.main(\"-m rsgcn -i rscgn_output_PKR -g 0 --data-name PKR --load-modelname rscgn_output_PKR/model_epoch-79\")\n",
    "\n",
    "datasets = ['CN']#['PKR', 'Negishi']\n",
    "for dataset in datasets:\n",
    "    best_epoch = get_best_epoch(output_name + dataset)\n",
    "    predict.main(\"-m {} -i {} -g 0 --data-name {} --load-modelname {}/model_epoch-{}\".format(\n",
    "        model_name, output_name + dataset, dataset, output_name + dataset, best_epoch\n",
    "    ).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pickle.load(open(output_name + \"suzuki/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(output_name + \"suzuki/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(output_name + \"suzuki/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/suzuki_type_test_v2.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagent_dict = dict()\n",
    "num_dicts = {'M': 28, 'L': 23, 'B': 35, 'S': 10, 'A': 17}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        reagent_dict[key+str(ii+1)] = unique_i\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [114, 115, 116, 117, 118]\n",
    "other_id = 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['B'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in gt_idx['B']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['B'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['B']:\n",
    "        pred_rank['B'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['B']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in pred_rank['B']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(model_name + \"_suzuki_prediction.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(output_name + \"CN/pred.pkl\", \"rb\"))  # Using 70%~ data\n",
    "gt = pickle.load(open(output_name + \"CN/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(output_name + \"CN/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/CN_coupling_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 44, 'L': 47, 'B': 13, 'S': 22, 'A': 74} # A has nan -> 18\n",
    "boundaries = [44, 91, 104, 126, 200]\n",
    "null_ids = [201, 202, 203, 204, 205]\n",
    "other_id = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['B'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in gt_idx['B']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['B'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['B']:\n",
    "        pred_rank['B'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['B']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in pred_rank['B']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(model_name + \"_CN_prediction.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negishi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(output_name + \"Negishi/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(output_name + \"Negishi/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(output_name + \"Negishi/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/Negishi_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 32, 'L': 20, 'T': 8, 'S': 10, 'A': 30}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [101, 102, 103, 104, 105]\n",
    "other_id = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['T'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in gt_idx['T']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['T'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['T']:\n",
    "        pred_rank['T'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['T']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in pred_rank['T']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(model_name + \"_negishi_prediction.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(output_name + \"PKR/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(output_name + \"PKR/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(output_name + \"PKR/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/PKR_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 18, 'L': 6, 'T': 7, 'S': 15, 'A': 11, 'G': 1, 'O': 13, 'P': 4}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [76, 77, 78, 79, 80, 81, 82]\n",
    "other_id = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    # gt_idx = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    # pred_rank = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    gt_idx = {}\n",
    "    pred_rank = {}\n",
    "    \n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    Gs = ids[(boundaries[4] <= ids) & (ids < boundaries[5])]\n",
    "    Os = ids[(boundaries[5] <= ids) & (ids < boundaries[6]) | (ids == null_ids[5])]\n",
    "    Ps = ids[(boundaries[6] <= ids) & (ids < boundaries[7]) | (ids == null_ids[6])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['T'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    gt_idx['G'] = [x for x in Gs]\n",
    "    gt_idx['O'] = [x for x in Os]\n",
    "    gt_idx['P'] = [x for x in Ps]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in gt_idx['T']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    gt_idx['G'] = [x + 1-boundaries[4] for x in gt_idx['G']]\n",
    "    gt_idx['O'] = [x + 1-boundaries[5] if x != null_ids[5] else num_dicts['O']+1 for x in gt_idx['O']]\n",
    "    gt_idx['P'] = [x + 1-boundaries[6] if x != null_ids[6] else num_dicts['P']+1 for x in gt_idx['P']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    pred_Gs = pred[i][boundaries[4]:boundaries[5]]\n",
    "    pred_Os = pred[i][boundaries[5]:boundaries[6]]\n",
    "    pred_Os = np.append(pred_Os, pred[i][null_ids[5]])\n",
    "    pred_Ps = pred[i][boundaries[6]:boundaries[7]]\n",
    "    pred_Ps = np.append(pred_Ps, pred[i][null_ids[6]])\n",
    "    \n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['T'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    pred_rank['G'] = pred_Gs\n",
    "    pred_rank['O'] = np.argsort(pred_Os)[-num_ranks:][::-1] + boundaries[5]\n",
    "    pred_rank['P'] = np.argsort(pred_Ps)[-4:][::-1] + boundaries[6]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['T']:\n",
    "        pred_rank['T'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['T']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    if boundaries[6] in pred_rank['O']:\n",
    "        pred_rank['O'] = [null_ids[5] if x == boundaries[6] else x for x in pred_rank['O']]\n",
    "\n",
    "    if boundaries[7] in pred_rank['O']:\n",
    "        pred_rank['P'] = [null_ids[6] if x == boundaries[7] else x for x in pred_rank['P']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in pred_rank['T']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "    pred_rank['O'] = [x + 1-boundaries[5] if x != null_ids[5] else num_dicts['O']+1 for x in pred_rank['O']]\n",
    "    pred_rank['P'] = [x + 1-boundaries[6] if x != null_ids[6] else num_dicts['P']+1 for x in pred_rank['P']]\n",
    "        \n",
    "    # gas class\n",
    "    if pred_rank['G'] > 0:\n",
    "        pred_rank['G'] = [1]\n",
    "    else:\n",
    "        pred_rank['G'] = []\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > 0:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(model_name + \"_PKR_prediction.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import accuracy, compute_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy for dict_keys(['M', 'L', 'B', 'S', 'A'])\n",
      "0.4891274238227147\n",
      "0.8770083102493075\n",
      "0.4166897506925208\n",
      "0.6505540166204986\n",
      "0.9524238227146814\n",
      "Top k accuracy for dict_keys(['M', 'L', 'B', 'S', 'A'])\n",
      "0.779224376731302\n",
      "0.9474376731301939\n",
      "0.7603185595567867\n",
      "0.8264542936288088\n",
      "0.9883656509695291\n"
     ]
    }
   ],
   "source": [
    "data_file = json.load(open(model_name+ \"_suzuki_prediction.json\", \"r\"))\n",
    "# type_count = {'M': 45, 'L': 48, 'B': 14, 'S': 23, 'A': 75}   # CN\n",
    "type_count = {'M': 29, 'L': 24, 'B': 36, 'S': 11, 'A': 18}  # suzuki\n",
    "# type_count = {'M': 33, 'L': 21, 'T': 9, 'S': 11, 'A': 31} # neigishi\n",
    "# type_count = {'M': 19, 'L': 7, 'T': 8, 'S': 16, 'A': 12, 'G': 1, 'O': 14, 'P': 5} # PKR\n",
    "top1_acc, topk_acc = accuracy(data_file, type_count, formatted_print=True)\n",
    "\n",
    "result = compute_PR(data_file, type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy for dict_keys(['M', 'L', 'B', 'S', 'A'])\n",
      "0.3304109589041096\n",
      "0.5197260273972603\n",
      "0.32273972602739726\n",
      "0.33452054794520547\n",
      "0.8904109589041096\n",
      "Top k accuracy for dict_keys(['M', 'L', 'B', 'S', 'A'])\n",
      "0.6794520547945205\n",
      "0.6934246575342465\n",
      "0.6827397260273973\n",
      "0.5884931506849315\n",
      "0.915068493150685\n"
     ]
    }
   ],
   "source": [
    "data_file = json.load(open(model_name+ \"_CN_prediction.json\", \"r\"))\n",
    "type_count = {'M': 45, 'L': 48, 'B': 14, 'S': 23, 'A': 75}   # CN\n",
    "# type_count = {'M': 29, 'L': 24, 'B': 36, 'S': 11, 'A': 18}  # suzuki\n",
    "# type_count = {'M': 33, 'L': 21, 'T': 9, 'S': 11, 'A': 31} # neigishi\n",
    "# type_count = {'M': 19, 'L': 7, 'T': 8, 'S': 16, 'A': 12, 'G': 1, 'O': 14, 'P': 5} # PKR\n",
    "top1_acc, topk_acc = accuracy(data_file, type_count, formatted_print=True)\n",
    "\n",
    "result = compute_PR(data_file, type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy for dict_keys(['M', 'L', 'T', 'S', 'A'])\n",
      "0.2886762360446571\n",
      "0.7878787878787878\n",
      "0.37320574162679426\n",
      "0.8149920255183413\n",
      "0.8309409888357256\n",
      "Top k accuracy for dict_keys(['M', 'L', 'T', 'S', 'A'])\n",
      "0.507177033492823\n",
      "0.8724082934609251\n",
      "0.6618819776714514\n",
      "0.8851674641148325\n",
      "0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "data_file = json.load(open(model_name+ \"_negishi_prediction.json\", \"r\"))\n",
    "# type_count = {'M': 45, 'L': 48, 'B': 14, 'S': 23, 'A': 75}   # CN\n",
    "# type_count = {'M': 29, 'L': 24, 'B': 36, 'S': 11, 'A': 18}  # suzuki\n",
    "type_count = {'M': 33, 'L': 21, 'T': 9, 'S': 11, 'A': 31} # neigishi\n",
    "# type_count = {'M': 19, 'L': 7, 'T': 8, 'S': 16, 'A': 12, 'G': 1, 'O': 14, 'P': 5} # PKR\n",
    "top1_acc, topk_acc = accuracy(data_file, type_count, formatted_print=True)\n",
    "\n",
    "result = compute_PR(data_file, type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy for dict_keys(['M', 'L', 'T', 'S', 'A', 'G', 'O', 'P'])\n",
      "0.7018867924528301\n",
      "0.9132075471698113\n",
      "0.6113207547169811\n",
      "0.6679245283018868\n",
      "0.7811320754716982\n",
      "0.8566037735849057\n",
      "0.9018867924528302\n",
      "0.8754716981132076\n",
      "Top k accuracy for dict_keys(['M', 'L', 'T', 'S', 'A', 'G', 'O', 'P'])\n",
      "0.9207547169811321\n",
      "0.9924528301886792\n",
      "0.8754716981132076\n",
      "0.879245283018868\n",
      "0.9773584905660377\n",
      "1.0\n",
      "0.9735849056603774\n",
      "0.9811320754716981\n"
     ]
    }
   ],
   "source": [
    "data_file = json.load(open(model_name+ \"_PKR_prediction.json\", \"r\"))\n",
    "# type_count = {'M': 45, 'L': 48, 'B': 14, 'S': 23, 'A': 75}   # CN\n",
    "# type_count = {'M': 29, 'L': 24, 'B': 36, 'S': 11, 'A': 18}  # suzuki\n",
    "# type_count = {'M': 33, 'L': 21, 'T': 9, 'S': 11, 'A': 31} # neigishi\n",
    "type_count = {'M': 19, 'L': 7, 'T': 8, 'S': 16, 'A': 12, 'G': 1, 'O': 14, 'P': 5} # PKR\n",
    "top1_acc, topk_acc = accuracy(data_file, type_count, formatted_print=True)\n",
    "\n",
    "result = compute_PR(data_file, type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metal\n",
    "ax = plt.subplot(111)\n",
    "x_range = [i-0.2 for i in range(0, len(result[0][0].keys()))]\n",
    "x_range2 = [i+0.2 for i in range(0, len(result[1][0].keys()))]\n",
    "ax.bar(x_range, result[0][0].values(), width=0.4, color='b', align='center')\n",
    "# ax.bar(x, z, width=0.2, color='g', align='center')\n",
    "ax.bar(x_range2, result[1][0].values(), width=0.4, color='r', align='center')\n",
    "# ax.xaxis_date()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend(['top-1', 'top-3'])\n",
    "plt.title(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_chainer_p36)",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
