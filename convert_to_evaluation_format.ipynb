{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #2.\n",
      "Duplicate key in file '/home/ubuntu/.config/matplotlib/matplotlibrc' line #3.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xlrd\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUZUKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(\"nfp/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(\"nfp/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(\"nfp/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/suzuki_type_test_v2.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reagent_dict = dict()\n",
    "num_dicts = {'M': 28, 'L': 23, 'B': 35, 'S': 10, 'A': 17}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        reagent_dict[key+str(ii+1)] = unique_i\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [114, 115, 116, 117, 118]\n",
    "other_id = 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['B'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in gt_idx['B']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['B'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['B']:\n",
    "        pred_rank['B'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['B']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in pred_rank['B']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(\"nfp_suzuki_prediction.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(\"CN_coupling/pred.pkl\", \"rb\"))  # Using 70%~ data\n",
    "gt = pickle.load(open(\"CN_coupling/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(\"CN_coupling/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/CN_coupling_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 44, 'L': 47, 'B': 13, 'S': 22, 'A': 74} # A has nan -> 18\n",
    "boundaries = [44, 91, 104, 126, 200]\n",
    "null_ids = [201, 202, 203, 204, 205]\n",
    "other_id = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'B': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['B'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in gt_idx['B']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['B'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['B']:\n",
    "        pred_rank['B'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['B']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['B'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['B']+1 for x in pred_rank['B']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(\"CN_dummy.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negishi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(\"negishi/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(\"negishi/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(\"negishi/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/Negishi_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 32, 'L': 20, 'T': 8, 'S': 10, 'A': 30}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [101, 102, 103, 104, 105]\n",
    "other_id = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    gt_idx = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    pred_rank = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "\n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['T'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in gt_idx['T']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['T'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['T']:\n",
    "        pred_rank['T'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['T']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in pred_rank['T']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > -5:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(\"Neigishi_dummy.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction\n",
    "pred = pickle.load(open(\"PKR/pred.pkl\", \"rb\")) \n",
    "gt = pickle.load(open(\"PKR/gt.pkl\", \"rb\"))\n",
    "test_ids = json.load(open(\"PKR/test_ids.json\", \"r\"))\n",
    "\n",
    "num_class = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_data = pd.read_csv('data/PKR_test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = {'M': 18, 'L': 6, 'T': 7, 'S': 15, 'A': 11, 'G': 1, 'O': 13, 'P': 4}\n",
    "unique_i = 0\n",
    "boundaries = []\n",
    "for key, num_r in num_dicts.items():\n",
    "    for ii in range(num_r):\n",
    "        unique_i += 1\n",
    "    boundaries.append(unique_i)\n",
    "null_ids = [76, 77, 78, 79, 80, 81, 82]\n",
    "other_id = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: pick top-1/5 reagents from each categorized reagent\n",
    "predictions = []\n",
    "\n",
    "num_ranks = 5\n",
    "\n",
    "for i in range(len(gt)):\n",
    "    i_id = int(test_ids[str(i)][1:-2])\n",
    "    i_pred = {'id': i_id}\n",
    "    \n",
    "    # gt_idx = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    # pred_rank = {'M': [], 'L': [], 'T': [], 'S': [], 'A': [], 'other': []}\n",
    "    gt_idx = {}\n",
    "    pred_rank = {}\n",
    "    \n",
    "    ids = np.where(gt[i][0] == 1)[0]\n",
    "    \n",
    "    Ms = ids[(ids < boundaries[0]) | (ids == null_ids[0])]\n",
    "    Ls = ids[(boundaries[0] <= ids) & (ids < boundaries[1]) | (ids == null_ids[1])]\n",
    "    Bs = ids[(boundaries[1] <= ids) & (ids < boundaries[2]) | (ids == null_ids[2])]\n",
    "    Ss = ids[(boundaries[2] <= ids) & (ids < boundaries[3]) | (ids == null_ids[3])]\n",
    "    As = ids[(boundaries[3] <= ids) & (ids < boundaries[4]) | (ids == null_ids[4])]\n",
    "    Gs = ids[(boundaries[4] <= ids) & (ids < boundaries[5])]\n",
    "    Os = ids[(boundaries[5] <= ids) & (ids < boundaries[6]) | (ids == null_ids[5])]\n",
    "    Ps = ids[(boundaries[6] <= ids) & (ids < boundaries[7]) | (ids == null_ids[6])]\n",
    "    other = ids[(ids == other_id)]\n",
    "    \n",
    "    gt_idx['M'] = [x for x in Ms]\n",
    "    gt_idx['L'] = [x for x in Ls]\n",
    "    gt_idx['T'] = [x for x in Bs]\n",
    "    gt_idx['S'] = [x for x in Ss]\n",
    "    gt_idx['A'] = [x for x in As]\n",
    "    gt_idx['G'] = [x for x in Gs]\n",
    "    gt_idx['O'] = [x for x in Os]\n",
    "    gt_idx['P'] = [x for x in Ps]\n",
    "    \n",
    "    gt_idx['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in gt_idx['M']]\n",
    "    gt_idx['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in gt_idx['L']]\n",
    "    gt_idx['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in gt_idx['T']]\n",
    "    gt_idx['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in gt_idx['S']]\n",
    "    gt_idx['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in gt_idx['A']]\n",
    "    gt_idx['G'] = [x + 1-boundaries[4] for x in gt_idx['G']]\n",
    "    gt_idx['O'] = [x + 1-boundaries[5] if x != null_ids[5] else num_dicts['O']+1 for x in gt_idx['O']]\n",
    "    gt_idx['P'] = [x + 1-boundaries[6] if x != null_ids[6] else num_dicts['P']+1 for x in gt_idx['P']]\n",
    "    \n",
    "    gt_idx['other'] = [x for x in other]\n",
    "        \n",
    "    pred_Ms = pred[i][:boundaries[0]]\n",
    "    pred_Ms = np.append(pred_Ms, pred[i][null_ids[0]])\n",
    "    pred_Ls = pred[i][boundaries[0]:boundaries[1]]\n",
    "    pred_Ls = np.append(pred_Ls, pred[i][null_ids[1]])\n",
    "    pred_Bs = pred[i][boundaries[1]:boundaries[2]]\n",
    "    pred_Bs = np.append(pred_Bs, pred[i][null_ids[2]])\n",
    "    pred_Ss = pred[i][boundaries[2]:boundaries[3]]\n",
    "    pred_Ss = np.append(pred_Ss, pred[i][null_ids[3]])\n",
    "    pred_As = pred[i][boundaries[3]:boundaries[4]]\n",
    "    pred_As = np.append(pred_As, pred[i][null_ids[4]])\n",
    "    pred_Gs = pred[i][boundaries[4]:boundaries[5]]\n",
    "    pred_Os = pred[i][boundaries[5]:boundaries[6]]\n",
    "    pred_Os = np.append(pred_Os, pred[i][null_ids[5]])\n",
    "    pred_Ps = pred[i][boundaries[6]:boundaries[7]]\n",
    "    pred_Ps = np.append(pred_Ps, pred[i][null_ids[6]])\n",
    "    \n",
    "    \n",
    "    pred_rank['M'] = np.argsort(pred_Ms)[-num_ranks:][::-1]\n",
    "    pred_rank['L'] = np.argsort(pred_Ls)[-num_ranks:][::-1] + boundaries[0]\n",
    "    pred_rank['T'] = np.argsort(pred_Bs)[-num_ranks:][::-1] + boundaries[1]\n",
    "    pred_rank['S'] = np.argsort(pred_Ss)[-num_ranks:][::-1] + boundaries[2]\n",
    "    pred_rank['A'] = np.argsort(pred_As)[-num_ranks:][::-1] + boundaries[3]\n",
    "    pred_rank['G'] = pred_Gs\n",
    "    pred_rank['O'] = np.argsort(pred_Os)[-num_ranks:][::-1] + boundaries[5]\n",
    "    pred_rank['P'] = np.argsort(pred_Ps)[-4:][::-1] + boundaries[6]\n",
    "    \n",
    "    # Null class.............\n",
    "    if boundaries[0] in pred_rank['M']:\n",
    "        pred_rank['M'] = [null_ids[0] if x == boundaries[0] else x for x in pred_rank['M']]\n",
    "    \n",
    "    if boundaries[1] in pred_rank['L']:\n",
    "        pred_rank['L'] = [null_ids[1] if x == boundaries[1] else x for x in pred_rank['L']]\n",
    "    \n",
    "    if boundaries[2] in pred_rank['T']:\n",
    "        pred_rank['T'] = [null_ids[2] if x == boundaries[2] else x for x in pred_rank['T']]\n",
    "    \n",
    "    if boundaries[3] in pred_rank['S']:\n",
    "        pred_rank['S'] = [null_ids[3] if x == boundaries[3] else x for x in pred_rank['S']]\n",
    "        \n",
    "    if boundaries[4] in pred_rank['A']:\n",
    "        pred_rank['A'] = [null_ids[4] if x == boundaries[4] else x for x in pred_rank['A']]\n",
    "        \n",
    "    if boundaries[6] in pred_rank['O']:\n",
    "        pred_rank['O'] = [null_ids[5] if x == boundaries[6] else x for x in pred_rank['O']]\n",
    "\n",
    "    if boundaries[7] in pred_rank['O']:\n",
    "        pred_rank['P'] = [null_ids[6] if x == boundaries[7] else x for x in pred_rank['P']]\n",
    "        \n",
    "    pred_rank['M'] = [x + 1 if x != null_ids[0] else num_dicts['M']+1 for x in pred_rank['M']]\n",
    "    pred_rank['L'] = [x + 1-boundaries[0] if x != null_ids[1] else num_dicts['L']+1 for x in pred_rank['L']]\n",
    "    pred_rank['T'] = [x + 1-boundaries[1] if x != null_ids[2] else num_dicts['T']+1 for x in pred_rank['T']]\n",
    "    pred_rank['S'] = [x + 1-boundaries[2] if x != null_ids[3] else num_dicts['S']+1 for x in pred_rank['S']]\n",
    "    pred_rank['A'] = [x + 1-boundaries[3] if x != null_ids[4] else num_dicts['A']+1 for x in pred_rank['A']]\n",
    "    pred_rank['O'] = [x + 1-boundaries[5] if x != null_ids[5] else num_dicts['O']+1 for x in pred_rank['O']]\n",
    "    pred_rank['P'] = [x + 1-boundaries[6] if x != null_ids[6] else num_dicts['P']+1 for x in pred_rank['P']]\n",
    "        \n",
    "    # gas class\n",
    "    if pred_rank['G'] > 0:\n",
    "        pred_rank['G'] = [1]\n",
    "    else:\n",
    "        pred_rank['G'] = []\n",
    "        \n",
    "    # Other class needs thresholding\n",
    "    other_score = pred[i][other_id]\n",
    "    pred_rank['other'] = []\n",
    "    if other_score > 0:\n",
    "        pred_rank['other'].append(other_id)\n",
    "        \n",
    "    i_pred['pred'] = pred_rank\n",
    "    i_pred['gt'] = gt_idx\n",
    "    \n",
    "    predictions.append(i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, datetime.datetime):\n",
    "        return obj.__str__()\n",
    "\n",
    "with open(\"PKR_dummy.json\", \"w\") as write_file:\n",
    "    json.dump(predictions, write_file, default=myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import accuracy, compute_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy for type  M  : 0.5762617859123683\n",
      "Top k accuracy for type  M  : 0.819814198557959\n",
      "Top 1 accuracy for type  L  : 0.884706045479756\n",
      "Top k accuracy for type  L  : 0.9541735995562951\n",
      "Top 1 accuracy for type  B  : 0.4636716583471991\n",
      "Top k accuracy for type  B  : 0.7795341098169717\n",
      "Top 1 accuracy for type  S  : 0.6656267332224071\n",
      "Top k accuracy for type  S  : 0.8484470327232391\n",
      "Top 1 accuracy for type  A  : 0.9559761508596784\n",
      "Top k accuracy for type  A  : 0.990363283416528\n"
     ]
    }
   ],
   "source": [
    "data_file = json.load(open(\"nfp_suzuki_prediction.json\", \"r\"))\n",
    "# type_count = {'M': 45, 'L': 48, 'B': 14, 'S': 23, 'A': 75}   # CN\n",
    "type_count = {'M': 29, 'L': 24, 'B': 36, 'S': 11, 'A': 18}  # suzuki\n",
    "# type_count = {'M': 33, 'L': 21, 'T': 9, 'S': 11, 'A': 31} # neigishi\n",
    "# type_count = {'M': 19, 'L': 7, 'T': 8, 'S': 16, 'A': 12, 'G': 1, 'O': 14, 'P': 5} # PKR\n",
    "top1_acc, topk_acc = accuracy(data_file, type_count)\n",
    "\n",
    "result = compute_PR(data_file, type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 0.022641509433962266, 'L': 0.12560646900269543, 'T': 0.045072115384615384, 'S': 0.0272383181511471, 'A': 0.05754716981132075, 'G': 0.9473684210526315, 'O': 0.0646900269541779, 'P': 0.16634615384615387, 'other': 0.2631578947368421, 'all': 0.06920852380571096}\n",
      "{'M': 0.041725281330283165, 'L': 0.13154680060092175, 'T': 0.09008077273190232, 'S': 0.04887293353576248, 'A': 0.08222525123282322, 'G': 0.9473684210526315, 'O': 0.06800359934669765, 'P': 0.19534833091436865, 'other': 0.2631578947368421, 'all': 0.08846075233636402}\n",
      "{'M': 0.05263157894736842, 'L': 0.14285714285714285, 'T': 0.10533707865168539, 'S': 0.06060376213592233, 'A': 0.08333333333333333, 'G': 0.2037735849056604, 'O': 0.07142857142857142, 'P': 0.1572727272727273, 'other': 0.05660377358490566, 'all': 0.08263973274990895}\n",
      "{'M': 0.1698483496877788, 'L': 0.336734693877551, 'T': 0.3283603344656389, 'S': 0.16308351922094255, 'A': 0.3095029239766082, 'G': 0.2037735849056604, 'O': 0.20408163265306123, 'P': 0.5647058823529412, 'other': 0.05660377358490566, 'all': 0.24669355662154546}\n",
      "{'M': 0.43018867924528303, 'L': 0.879245283018868, 'T': 0.2830188679245283, 'S': 0.3320754716981132, 'A': 0.690566037735849, 'G': 0.9473684210526315, 'O': 0.9056603773584906, 'P': 0.6528301886792452, 'other': 0.2631578947368421, 'all': 0.5967496190959878}\n",
      "{'M': 0.23773584905660378, 'L': 0.30062893081761005, 'T': 0.19874213836477989, 'S': 0.20125786163522014, 'A': 0.2943396226415094, 'G': 0.9473684210526315, 'O': 0.31069182389937106, 'P': 0.32075471698113206, 'other': 0.2631578947368421, 'all': 0.2731114632857897}\n",
      "{'M': 0.43018867924528303, 'L': 0.879245283018868, 'T': 0.2830188679245283, 'S': 0.3024054982817869, 'A': 0.690566037735849, 'G': 0.2037735849056604, 'O': 0.898876404494382, 'P': 0.6528301886792452, 'other': 0.05660377358490566, 'all': 0.4869457107335267}\n",
      "{'M': 0.7132075471698113, 'L': 0.9018867924528302, 'T': 0.5962264150943396, 'S': 0.5498281786941581, 'A': 0.8830188679245283, 'G': 0.2037735849056604, 'O': 0.9250936329588015, 'P': 0.9622641509433962, 'other': 0.05660377358490566, 'all': 0.6427683381682553}\n"
     ]
    }
   ],
   "source": [
    "print(result[0][2])\n",
    "print(result[1][2])\n",
    "\n",
    "print(result[0][3])\n",
    "print(result[1][3])\n",
    "\n",
    "print(result[0][4])\n",
    "print(result[1][4])\n",
    "\n",
    "print(result[0][5])\n",
    "print(result[1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metal\n",
    "ax = plt.subplot(111)\n",
    "x_range = [i-0.2 for i in range(0, len(result[0][0].keys()))]\n",
    "x_range2 = [i+0.2 for i in range(0, len(result[1][0].keys()))]\n",
    "ax.bar(x_range, result[0][0].values(), width=0.4, color='b', align='center')\n",
    "# ax.bar(x, z, width=0.2, color='g', align='center')\n",
    "ax.bar(x_range2, result[1][0].values(), width=0.4, color='r', align='center')\n",
    "# ax.xaxis_date()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend(['top-1', 'top-3'])\n",
    "plt.title(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_chainer)",
   "language": "python",
   "name": "conda_chainer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
